{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "beb52e44",
      "metadata": {
        "id": "beb52e44"
      },
      "source": [
        "\n",
        "# Neuro‑Symbolic AI Tutorial: Echo State Networks + Symbolic Rules (CartPole)\n",
        "\n",
        "This is a **tutorial** that shows how to combine:\n",
        "\n",
        "- **Neural computation** (an **Echo State Network**, ESN)  \n",
        "- with **symbolic reasoning** (human‑interpretable **if/else rules**)  \n",
        "\n",
        "…to build a simple **neuro‑symbolic policy** for the classic control task **CartPole**.\n",
        "\n",
        "---\n",
        "\n",
        "## Learning goals\n",
        "\n",
        "By the end, students should be able to:\n",
        "\n",
        "1. Explain what a **reservoir / ESN** is and why we often **freeze** its recurrent weights.\n",
        "2. Implement a **symbolic module** using rules.\n",
        "3. Combine neural and symbolic features into a single policy.\n",
        "4. Train a policy using **REINFORCE** (policy‑gradient).\n",
        "5. Understand limitations and how to improve the approach.\n",
        "\n",
        "---\n",
        "\n",
        "## What we will build\n",
        "\n",
        "We will build a policy:\n",
        "\n",
        "\\[\n",
        "\\pi(a \\mid s) = \\text{Softmax}(\\text{Readout}([\\text{ESN}(s), \\ \\text{Rules}(s)]))\n",
        "\\]\n",
        "\n",
        "- **ESN(s)**: high‑dimensional reservoir state  \n",
        "- **Rules(s)**: small interpretable vector derived from symbolic rules  \n",
        "- **Readout**: a trainable linear layer mapping combined features to action logits\n",
        "\n",
        "---\n",
        "\n",
        "## Note about Gym versions\n",
        "\n",
        "Colab sometimes ships different versions of `gym` / `gymnasium`.\n",
        "This notebook includes **compatibility code** to handle the \"new step API\" style outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "417ac69c",
      "metadata": {
        "id": "417ac69c"
      },
      "source": [
        "\n",
        "## 1) Setup\n",
        "\n",
        "This cell:\n",
        "- configures minor compatibility hacks for `gym`,\n",
        "- enables TF32 (optional, for speed on newer GPUs),\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf56afb8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf56afb8",
        "outputId": "b32ea77e-eb5a-42ee-b770-b80913e5ae8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.9.0+cu128\n",
            "Gym: 0.25.2\n",
            "CUDA available: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# --- Basic imports ---\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# Gym checker compatibility: some versions still reference np.bool8\n",
        "# (np.bool8 was deprecated; aliasing avoids noisy errors in older Gym checks)\n",
        "np.bool8 = np.bool_\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import gym\n",
        "\n",
        "# --- Optional GPU performance knobs (safe to leave on/off) ---\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# Free cached GPU memory (optional)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"Gym:\", gym.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27735f49",
      "metadata": {
        "id": "27735f49"
      },
      "source": [
        "\n",
        "## 2) Background: Echo State Networks (ESN)\n",
        "\n",
        "An **Echo State Network** is a type of **reservoir computing** model:\n",
        "\n",
        "- It has a recurrent “reservoir” with weights **W**.\n",
        "- The reservoir weights are typically **fixed** (not trained), but chosen so that the system is stable.\n",
        "- Only a **readout layer** is trained (e.g., linear classifier/regressor).\n",
        "\n",
        "### The \"echo state property\" (intuition)\n",
        "\n",
        "We want the reservoir state to depend on the **recent history** of inputs, but not explode.\n",
        "A common trick is to scale the reservoir recurrent matrix so that its **spectral radius** is < 1.\n",
        "\n",
        "### In this tutorial\n",
        "\n",
        "We will:\n",
        "- Initialize a sparse random reservoir matrix **W**,\n",
        "- scale it to a chosen spectral radius,\n",
        "- compute reservoir states with:\n",
        "\n",
        "\\[\n",
        "h_t = \\tanh(W_{\\text{in}} x_t + W h_{t-1})\n",
        "\\]\n",
        "\n",
        "Then we concatenate **h_t** with a symbolic vector and pass it to a trainable readout.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23241079",
      "metadata": {
        "id": "23241079"
      },
      "source": [
        "\n",
        "## 3) Symbolic reasoning module (rules)\n",
        "\n",
        "A symbolic component is usually:\n",
        "- **interpretable**\n",
        "- **editable**\n",
        "- potentially **verifiable**\n",
        "\n",
        "Here we define rules based on two CartPole observations:\n",
        "\n",
        "- **cart position** \\(x\\)\n",
        "- **pole angle** \\(\\theta\\)\n",
        "\n",
        "The output is a small vector of size 2 that you can interpret as:\n",
        "- tendency toward action 0 vs action 1 (left vs right)\n",
        "\n",
        "> This is not the only way to encode rules — it’s just a simple, teachable example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "969f7654",
      "metadata": {
        "id": "969f7654"
      },
      "outputs": [],
      "source": [
        "class SymbolicReasoningModule:\n",
        "    '''\n",
        "    A tiny rule-based module.\n",
        "\n",
        "    Input:  CartPole state vector: [x, x_dot, theta, theta_dot]\n",
        "    Output: A 2D vector (symbolic preference for action 0 vs 1)\n",
        "\n",
        "    Rules are intentionally simple so students can:\n",
        "    - read them\n",
        "    - modify them\n",
        "    - see how behavior changes\n",
        "    '''\n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "\n",
        "        # Rule functions return [pref_action_0, pref_action_1]\n",
        "        self.rules = {\n",
        "            \"pole_angle\": lambda angle: (\n",
        "                [0.9, 0.1] if angle < -0.1 else\n",
        "                ([0.1, 0.9] if angle > 0.1 else [0.5, 0.5])\n",
        "            ),\n",
        "            \"cart_position\": lambda pos: (\n",
        "                [0.8, 0.2] if pos < -1.0 else\n",
        "                ([0.2, 0.8] if pos > 1.0 else [0.5, 0.5])\n",
        "            )\n",
        "        }\n",
        "\n",
        "    def forward(self, state):\n",
        "        # CartPole: state = [x, x_dot, theta, theta_dot]\n",
        "        pole_angle = float(state[2].item())\n",
        "        cart_position = float(state[0].item())\n",
        "\n",
        "        angle_output = self.rules[\"pole_angle\"](pole_angle)\n",
        "        position_output = self.rules[\"cart_position\"](cart_position)\n",
        "\n",
        "        # Combine rule outputs (simple average)\n",
        "        symbolic_output = [(a + b) / 2.0 for a, b in zip(angle_output, position_output)]\n",
        "        return torch.tensor(symbolic_output, dtype=torch.float32, device=self.device)\n",
        "\n",
        "    def refine_rules(self, feedback):\n",
        "        '''\n",
        "        Very simple rule adaptation:\n",
        "        - if a rule family gets negative feedback, soften it.\n",
        "\n",
        "        This is *not* a principled approach; it's a teaching example.\n",
        "        '''\n",
        "        for key in self.rules:\n",
        "            if feedback.get(key, 0) < 0:\n",
        "                # Use a \"softer\" preference\n",
        "                self.rules[key] = lambda x: (\n",
        "                    [0.6, 0.4] if x < -0.1 else\n",
        "                    ([0.4, 0.6] if x > 0.1 else [0.5, 0.5])\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93a474dc",
      "metadata": {
        "id": "93a474dc"
      },
      "source": [
        "\n",
        "## 4) Neuro‑symbolic ESN module\n",
        "\n",
        "We combine:\n",
        "- reservoir state \\(h_t\\) of size `reservoir_dim`\n",
        "- symbolic output of size `symbolic_dim=2`\n",
        "\n",
        "Then the **readout** maps the concatenated vector to action logits.\n",
        "\n",
        "### Why this is \"neuro‑symbolic\"\n",
        "- The reservoir learns a rich nonlinear representation of the state.\n",
        "- The symbolic rules encode human intuition.\n",
        "- The final decision uses **both**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d2ad86c",
      "metadata": {
        "id": "5d2ad86c"
      },
      "outputs": [],
      "source": [
        "class NeuroSymbolicEchoStateNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, reservoir_dim, output_dim, device,\n",
        "                 symbolic_dim=2, spectral_radius=0.9, sparsity=0.1):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.reservoir_dim = reservoir_dim\n",
        "        self.symbolic_dim = symbolic_dim\n",
        "\n",
        "        # Input-to-reservoir weights (fixed)\n",
        "        self.Win = torch.randn(reservoir_dim, input_dim, device=device) * 0.1\n",
        "\n",
        "        # Reservoir recurrent weights: sparse + scaled by spectral radius\n",
        "        W = np.random.randn(reservoir_dim, reservoir_dim)\n",
        "        W *= (np.random.rand(reservoir_dim, reservoir_dim) < sparsity)\n",
        "\n",
        "        # Scale by spectral radius\n",
        "        eigs = np.max(np.abs(np.linalg.eigvals(W)))\n",
        "        W = (W / (eigs + 1e-12)) * spectral_radius\n",
        "\n",
        "        # Store as torch tensor\n",
        "        self.W = torch.from_numpy(W.astype(np.float32)).to(device)\n",
        "\n",
        "        # Reservoir state (updated every forward pass)\n",
        "        self.state = torch.zeros(reservoir_dim, device=device, dtype=torch.float32)\n",
        "\n",
        "        # Symbolic module + trainable readout\n",
        "        self.symbolic_module = SymbolicReasoningModule(device=device)\n",
        "        self.readout = nn.Linear(reservoir_dim + symbolic_dim, output_dim).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: state vector (shape: [input_dim])\n",
        "        returns: action logits (shape: [output_dim])\n",
        "        '''\n",
        "        self.state = torch.tanh(self.Win @ x + self.W @ self.state)\n",
        "        symbolic_output = self.symbolic_module.forward(x)\n",
        "        combined = torch.cat((self.state, symbolic_output))\n",
        "        return self.readout(combined)\n",
        "\n",
        "    def refine_symbolic_rules(self, feedback):\n",
        "        self.symbolic_module.refine_rules(feedback)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3d7ce98",
      "metadata": {
        "id": "d3d7ce98"
      },
      "source": [
        "\n",
        "## 5) Policy network\n",
        "\n",
        "We convert ESN logits into a probability distribution using Softmax:\n",
        "\n",
        "\\[\n",
        "\\pi(a|s) = \\text{Softmax}(\\text{logits})\n",
        "\\]\n",
        "\n",
        "Then we sample actions using a categorical distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c8b3c2d",
      "metadata": {
        "id": "7c8b3c2d"
      },
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, esn):\n",
        "        super().__init__()\n",
        "        self.esn = esn\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.esn(x)\n",
        "        return self.softmax(logits)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d56e6892",
      "metadata": {
        "id": "d56e6892"
      },
      "source": [
        "\n",
        "## 6) Training with REINFORCE (policy gradient)\n",
        "\n",
        "We use the classic REINFORCE update:\n",
        "\n",
        "\\[\n",
        "\\nabla_\\theta J(\\theta) \\approx \\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\, G_t\n",
        "\\]\n",
        "\n",
        "where \\(G_t\\) is the discounted return from time \\(t\\).\n",
        "\n",
        "### In code\n",
        "\n",
        "- collect `log_probs` during the episode,\n",
        "- compute discounted returns,\n",
        "- normalize returns (helps stability),\n",
        "- compute loss = `-log_prob * return`,\n",
        "- update via Adam.\n",
        "\n",
        "We also compute a simple **feedback signal** for rule refinement:\n",
        "- reward is positive when the pole is near upright and cart near center,\n",
        "- otherwise give negative points.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "078e42fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "078e42fd",
        "outputId": "1d71b5ab-18e8-484b-da6b-630caa812c90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 000 | Total Reward: 21.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 010 | Total Reward: 9.0\n",
            "Episode 020 | Total Reward: 12.0\n",
            "Episode 030 | Total Reward: 8.0\n",
            "Episode 040 | Total Reward: 28.0\n",
            "Episode 050 | Total Reward: 10.0\n",
            "Episode 060 | Total Reward: 47.0\n",
            "Episode 070 | Total Reward: 19.0\n",
            "Episode 080 | Total Reward: 85.0\n",
            "Episode 090 | Total Reward: 60.0\n",
            "Episode 100 | Total Reward: 48.0\n",
            "Episode 110 | Total Reward: 54.0\n",
            "Episode 120 | Total Reward: 55.0\n",
            "Episode 130 | Total Reward: 95.0\n",
            "Episode 140 | Total Reward: 65.0\n",
            "Episode 150 | Total Reward: 86.0\n",
            "Episode 160 | Total Reward: 143.0\n",
            "Episode 170 | Total Reward: 93.0\n",
            "Episode 180 | Total Reward: 123.0\n",
            "Episode 190 | Total Reward: 128.0\n",
            "Episode 200 | Total Reward: 173.0\n",
            "Episode 210 | Total Reward: 195.0\n",
            "Episode 220 | Total Reward: 70.0\n",
            "Episode 230 | Total Reward: 123.0\n",
            "Episode 240 | Total Reward: 175.0\n",
            "Episode 250 | Total Reward: 413.0\n",
            "Episode 260 | Total Reward: 148.0\n",
            "Episode 270 | Total Reward: 118.0\n",
            "Episode 280 | Total Reward: 88.0\n",
            "Episode 290 | Total Reward: 133.0\n",
            "Episode 300 | Total Reward: 45.0\n",
            "Episode 310 | Total Reward: 315.0\n",
            "Episode 320 | Total Reward: 192.0\n",
            "Episode 330 | Total Reward: 500.0\n",
            "Episode 340 | Total Reward: 500.0\n",
            "Episode 350 | Total Reward: 500.0\n",
            "Episode 360 | Total Reward: 500.0\n",
            "Episode 370 | Total Reward: 500.0\n",
            "Episode 380 | Total Reward: 399.0\n",
            "Episode 390 | Total Reward: 203.0\n",
            "Episode 400 | Total Reward: 152.0\n",
            "Episode 410 | Total Reward: 163.0\n",
            "Episode 420 | Total Reward: 156.0\n",
            "Episode 430 | Total Reward: 165.0\n",
            "Episode 440 | Total Reward: 147.0\n",
            "Episode 450 | Total Reward: 241.0\n",
            "Episode 460 | Total Reward: 251.0\n",
            "Episode 470 | Total Reward: 481.0\n",
            "Episode 480 | Total Reward: 500.0\n",
            "Episode 490 | Total Reward: 500.0\n",
            "Episode 500 | Total Reward: 500.0\n",
            "Episode 510 | Total Reward: 500.0\n",
            "Episode 520 | Total Reward: 500.0\n",
            "Episode 530 | Total Reward: 500.0\n",
            "Episode 540 | Total Reward: 482.0\n",
            "Episode 550 | Total Reward: 372.0\n",
            "Episode 560 | Total Reward: 389.0\n",
            "Episode 570 | Total Reward: 500.0\n",
            "Episode 580 | Total Reward: 500.0\n",
            "Episode 590 | Total Reward: 500.0\n",
            "Episode 600 | Total Reward: 500.0\n",
            "Episode 610 | Total Reward: 500.0\n",
            "Episode 620 | Total Reward: 500.0\n",
            "Episode 630 | Total Reward: 500.0\n",
            "Episode 640 | Total Reward: 500.0\n",
            "Episode 650 | Total Reward: 500.0\n",
            "Episode 660 | Total Reward: 500.0\n",
            "Episode 670 | Total Reward: 500.0\n",
            "Episode 680 | Total Reward: 500.0\n",
            "Episode 690 | Total Reward: 500.0\n",
            "Episode 700 | Total Reward: 500.0\n",
            "Episode 710 | Total Reward: 500.0\n",
            "Episode 720 | Total Reward: 500.0\n",
            "Episode 730 | Total Reward: 500.0\n",
            "Episode 740 | Total Reward: 165.0\n",
            "Episode 750 | Total Reward: 500.0\n",
            "Episode 760 | Total Reward: 500.0\n",
            "Episode 770 | Total Reward: 500.0\n",
            "Episode 780 | Total Reward: 500.0\n",
            "Episode 790 | Total Reward: 500.0\n",
            "Episode 800 | Total Reward: 500.0\n",
            "Episode 810 | Total Reward: 500.0\n",
            "Episode 820 | Total Reward: 500.0\n",
            "Episode 830 | Total Reward: 500.0\n",
            "Episode 840 | Total Reward: 500.0\n",
            "Episode 850 | Total Reward: 500.0\n",
            "Episode 860 | Total Reward: 500.0\n",
            "Episode 870 | Total Reward: 500.0\n",
            "Episode 880 | Total Reward: 500.0\n",
            "Episode 890 | Total Reward: 500.0\n",
            "Episode 900 | Total Reward: 500.0\n",
            "Episode 910 | Total Reward: 500.0\n",
            "Episode 920 | Total Reward: 500.0\n",
            "Episode 930 | Total Reward: 474.0\n",
            "Episode 940 | Total Reward: 500.0\n",
            "Episode 950 | Total Reward: 500.0\n",
            "Episode 960 | Total Reward: 500.0\n",
            "Episode 970 | Total Reward: 500.0\n",
            "Episode 980 | Total Reward: 500.0\n",
            "Episode 990 | Total Reward: 500.0\n",
            "Episode 1000 | Total Reward: 500.0\n",
            "Episode 1010 | Total Reward: 500.0\n",
            "Episode 1020 | Total Reward: 500.0\n",
            "Episode 1030 | Total Reward: 500.0\n",
            "Episode 1040 | Total Reward: 500.0\n",
            "Episode 1050 | Total Reward: 500.0\n",
            "Episode 1060 | Total Reward: 500.0\n",
            "Episode 1070 | Total Reward: 500.0\n",
            "Episode 1080 | Total Reward: 500.0\n",
            "Episode 1090 | Total Reward: 500.0\n",
            "Episode 1100 | Total Reward: 487.0\n",
            "Episode 1110 | Total Reward: 500.0\n",
            "Episode 1120 | Total Reward: 500.0\n",
            "Episode 1130 | Total Reward: 500.0\n",
            "Episode 1140 | Total Reward: 500.0\n",
            "Episode 1150 | Total Reward: 500.0\n",
            "Episode 1160 | Total Reward: 500.0\n",
            "Episode 1170 | Total Reward: 500.0\n",
            "Episode 1180 | Total Reward: 500.0\n",
            "Episode 1190 | Total Reward: 500.0\n",
            "Episode 1200 | Total Reward: 500.0\n",
            "Episode 1210 | Total Reward: 500.0\n",
            "Episode 1220 | Total Reward: 500.0\n",
            "Episode 1230 | Total Reward: 500.0\n",
            "Episode 1240 | Total Reward: 500.0\n",
            "Episode 1250 | Total Reward: 500.0\n",
            "Episode 1260 | Total Reward: 500.0\n",
            "Episode 1270 | Total Reward: 500.0\n",
            "Episode 1280 | Total Reward: 500.0\n",
            "Episode 1290 | Total Reward: 500.0\n",
            "Episode 1300 | Total Reward: 500.0\n",
            "Episode 1310 | Total Reward: 500.0\n",
            "Episode 1320 | Total Reward: 500.0\n",
            "Episode 1330 | Total Reward: 500.0\n",
            "Episode 1340 | Total Reward: 500.0\n",
            "Episode 1350 | Total Reward: 500.0\n",
            "Episode 1360 | Total Reward: 500.0\n",
            "Episode 1370 | Total Reward: 500.0\n",
            "Episode 1380 | Total Reward: 500.0\n",
            "Episode 1390 | Total Reward: 500.0\n",
            "Episode 1400 | Total Reward: 500.0\n",
            "Episode 1410 | Total Reward: 500.0\n",
            "Episode 1420 | Total Reward: 500.0\n",
            "Episode 1430 | Total Reward: 500.0\n",
            "Episode 1440 | Total Reward: 500.0\n",
            "Episode 1450 | Total Reward: 319.0\n",
            "Episode 1460 | Total Reward: 500.0\n",
            "Episode 1470 | Total Reward: 500.0\n",
            "Episode 1480 | Total Reward: 500.0\n",
            "Episode 1490 | Total Reward: 500.0\n"
          ]
        }
      ],
      "source": [
        "def make_env():\n",
        "    '''\n",
        "    Create CartPole with compatibility across Gym versions.\n",
        "\n",
        "    Some versions accept new_step_api=True, others don't.\n",
        "    We'll try it and fall back if needed.\n",
        "    '''\n",
        "    try:\n",
        "        return gym.make(\"CartPole-v1\", new_step_api=True)\n",
        "    except TypeError:\n",
        "        return gym.make(\"CartPole-v1\")\n",
        "\n",
        "\n",
        "def reset_env(env):\n",
        "    '''Gym reset compatibility: returns obs or (obs, info).'''\n",
        "    out = env.reset()\n",
        "    obs = out[0] if isinstance(out, tuple) else out\n",
        "    return obs\n",
        "\n",
        "\n",
        "def step_env(env, action):\n",
        "    '''\n",
        "    Gym step compatibility:\n",
        "    - new API: obs, reward, terminated, truncated, info\n",
        "    - old API: obs, reward, done, info\n",
        "    '''\n",
        "    out = env.step(action)\n",
        "    if len(out) == 5:\n",
        "        obs, reward, terminated, truncated, info = out\n",
        "        done = terminated or truncated\n",
        "        return obs, reward, done, info\n",
        "    else:\n",
        "        obs, reward, done, info = out\n",
        "        return obs, reward, done, info\n",
        "\n",
        "\n",
        "def train(\n",
        "    episodes=300,\n",
        "    reservoir_dim=150,\n",
        "    lr=1e-2,\n",
        "    gamma=0.99,\n",
        "    print_every=10,\n",
        "    seed=0\n",
        "):\n",
        "    # Reproducibility (partial; environments can still add randomness)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    env = make_env()\n",
        "\n",
        "    input_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    esn = NeuroSymbolicEchoStateNetwork(\n",
        "        input_dim=input_dim,\n",
        "        reservoir_dim=reservoir_dim,\n",
        "        output_dim=action_dim,\n",
        "        device=device\n",
        "    )\n",
        "    policy = PolicyNetwork(esn).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(policy.parameters(), lr=lr)\n",
        "\n",
        "    reward_history = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        obs = reset_env(env)\n",
        "        state = torch.tensor(obs, dtype=torch.float32, device=device)\n",
        "\n",
        "        rewards = []\n",
        "        log_probs = []\n",
        "\n",
        "        # feedback for symbolic rule refinement\n",
        "        feedback = {\"pole_angle\": 0.0, \"cart_position\": 0.0}\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            probs = policy(state)\n",
        "            dist = torch.distributions.Categorical(probs)\n",
        "            action = dist.sample()\n",
        "\n",
        "            log_probs.append(dist.log_prob(action))\n",
        "\n",
        "            obs, reward, done, _info = step_env(env, int(action.item()))\n",
        "            next_state = torch.tensor(obs, dtype=torch.float32, device=device)\n",
        "            rewards.append(float(reward))\n",
        "\n",
        "            # Simple heuristic feedback signal:\n",
        "            # Encourage upright pole (small angle) and centered cart (small position).\n",
        "            feedback[\"pole_angle\"] += reward if abs(float(next_state[2])) < 0.1 else -1.0\n",
        "            feedback[\"cart_position\"] += reward if abs(float(next_state[0])) < 1.0 else -1.0\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        # refine rules after each episode\n",
        "        esn.refine_symbolic_rules(feedback)\n",
        "\n",
        "        # discounted returns\n",
        "        discounted = []\n",
        "        R = 0.0\n",
        "        for r in reversed(rewards):\n",
        "            R = r + gamma * R\n",
        "            discounted.insert(0, R)\n",
        "\n",
        "        returns = torch.tensor(discounted, dtype=torch.float32, device=device)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "        # policy gradient loss\n",
        "        loss = torch.stack([-lp * Rt for lp, Rt in zip(log_probs, returns)]).sum()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_reward = sum(rewards)\n",
        "        reward_history.append(total_reward)\n",
        "\n",
        "        if episode % print_every == 0:\n",
        "            print(f\"Episode {episode:03d} | Total Reward: {total_reward:.1f}\")\n",
        "\n",
        "    env.close()\n",
        "    return reward_history\n",
        "\n",
        "\n",
        "# Run training (feel free to change episodes)\n",
        "reward_history = train(episodes=1500, print_every=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1895bb0d",
      "metadata": {
        "id": "1895bb0d"
      },
      "source": [
        "\n",
        "## 7) Plot learning curve\n",
        "\n",
        "We can plot total episode reward over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9f44568",
      "metadata": {
        "id": "e9f44568"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(reward_history)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"Neuro-Symbolic ESN on CartPole (REINFORCE)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86a24058",
      "metadata": {
        "id": "86a24058"
      },
      "source": [
        "\n",
        "## 8) Discussion: what worked, what didn’t, and why\n",
        "\n",
        "### What’s nice here\n",
        "- **Interpretability:** the symbolic module is readable and editable.\n",
        "- **Modularity:** you can replace rules or reservoir without changing the whole system.\n",
        "- **Fast iteration:** ESN training is light because only the readout is trained.\n",
        "\n",
        "### Limitations of this tutorial design\n",
        "- The ESN reservoir weights are random and fixed: performance can vary by seed.\n",
        "- The rule refinement method here is **heuristic** and not guaranteed to improve.\n",
        "- REINFORCE has high variance; it may need tuning.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}